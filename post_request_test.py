#encoding=utf-8
import json

import requests

url = 'http://127.0.0.1:38081/predict'
data = {"instances": json.dumps([
    'dissecting a logic-based system. this can be alleviated by placing restrictions on the form of meaning postulates and input formulas and using heuristic search methods .. although such an approach was applied with some success in a limited-domain system translating logical forms into database queries ( rayner and alshawi 1992 ) , it is likely to be impractical for language translation with tens of thousands of sense predicates and related axioms .. setting aside the intractability issue , this approach does not offer a principled way of choosing between alternative solutions proposed by the prover .	4',
    '7. disambiguating capitalized words. the best performance on the wsj corpus was achieved by a combination of the satz system (palmer and hearst 1997) with the alembic system (aberdeen et al.. 1995): a 0.5% error rate.. the best performance on the brown corpus, a 0.2% error rate, was reported by riley (1989), who trained a decision tree classifier on a 25-million-word corpus.	4',
    '1. introduction. watanabe (1993) combines lexical and dependency mappings to form his generalizations.. other similar approaches include those of cicekli and g¨uvenir (1996), mctait and trujillo (1999), carl (1999), and brown (2000), inter alia.. in our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <det> the good man: le bon homme in such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon.	0',
    '1 introduction. thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship (church, 1988; ramshaw and marcus, 1995; argamon et al., 1998; cardie and pierce, 1998; munoz et al., 1999; punyakanok and roth, 2001; buchholz et al., 1999; tjong kim sang and buchholz, 2000).. research on shallow parsing was inspired by psycholinguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g., conversational) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint.. first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such	0',
    'conclusion. clearly , a central task of future work is a further exploration of the relation between complete-data and incomplete-data estimation for larger , manually disambiguated treebanks .. an interesting question is whether a systematic variation of training data size along the lines of the em - experiments of ( nigam et al. 2000 ) for text classification will show similar results , namely a systematic dependence of the relative gain due to em training from the relative sizes of unannotated and annotated data .. furthermore , it is important to show that em - based methods can be applied successfully also to other statistical parsing frameworks .	2',
    '4. methodology. many lexicons, both automatically acquired and manually created, are more fine grained in their approaches to subcategorized clausal arguments, differentiating, for example, between a that-clause and a to + infinitive clause (ushioda et al.. 1993).. with only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.	0',
    'rules for the allocation and transfer of control. . we use the framework for the allocation and transfer of control of whittaker and stenton ( 1988 ) .. the analysis is based on a classification of utterances into 4 types .	2',
    '1 introduction. mcknight and srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.. building on the work of ruch et al.. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models (hmms); cf.	0',
    '8. related research. 2000; jijkoun and de rijke 2005; soricut and brill 2006).. an important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.. in fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.	4',
    '2 combinatory categorial grammar. figure 1: ccg parse for “the man walks to work.” we follow lewis and steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.. we further add rules for combining with punctuation to the left and right and allow for the merge rule x → x x of clark and curran (2007).. 3 generative model in this section, we present our novel supertagcontext model (scm) that augments a standard pcfg with parameters governing the supertags to the left and right of each constituent.	2',
    '4 related work. the first direct application of parse forest in translation is our previous work (mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below).. this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.. 11 · e e fra�g7 11 · v e yield(frag) p(e) (2) β(v) 211 bleu score our experiments will use both default 1-best decoding and forest-based decoding.	3',
    'discussion. the ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors .. this is discussed in the context of combining part-of-speech taggers in ( brill and wu 1998 ) .. they provide a measure for assessing the complementarity of errors between two taggers that could be adapted for use with larger ensembles such as the one discussed here , which has nine members .	2',
    '3 the experiments. to quantify the relative strengths of these transitive inferences, shaw and hatzivassiloglou (1999) propose to assign a weight to each link.. say the order (a,b) occurs m times and the pair {a,b} occurs n times in total.. then the weight of the pair a —* b is: n 1 n n ∑ k 2 k=m this weight decreases as the probability that the observed order did not occur strictly by chance increases.	0',
    'language representation (i). boundary markers can be considered invisible tags , or hypertags , which have probabilistic relationships with adjacent tags in the same way that words do .. atwell ( 1987 ) and church ( 1989 ) have used this approach .. if embedded syntactic constituents are sought in a single pass , this can lead to computation al overload ( pocock and atwell 1994 ) .	4',
    'introduction. moreover , only one word error was considered though several word errors can occur simultaneously in the running text .. a general algorithm for least-errors recognition ( lyon 1974 ) , proposed by g. lyon ( 0000 ) , is to find out the least number of errors necessary to successful parsing and recover them .. because this algorithm is also syntactically oriented and based on a chart , it has the same advantage as that of mellish ( 0000 )  parser .	4',
    'the framework\'s representations. figure 2 illustrates a dsynts from a meteorological application, meteocogent (kittredge and lavoie, 1998), represented using the standard graphical notation and also the realpro ascii notation used internally in the framework (lavoie and rambow, 1997).. as figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ascii notation supported in the framework.. this also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach.',
    'background: the andes physics tutor. robust natural language understanding in atlas-andes is provided by rosé\'s carmel system (rosé 2000); it uses the spelling correction algorithm devised by elmi and evens (1998).. 5.2 structure of human tutorial dialogues in an earlier analysis (kim, freedman and evens 1998) we showed that a significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure of task-oriented dialogues (grosz and sidner 1986).. furthermore, a main building block of the discourse hierarchy, corresponding to the transaction level in conversation analysis (sinclair and coulthard 1975), matches the tutoring episode defined by vanlehn et al.',
    ' One of the valuable indicators of the structure of text is lexical cohesion  ( Halliday and Hasan 1976 )  . ',
    ' Magic ( templates ) is a general compilation technique for efficient bottom-up evaluation of logic programs developed in the deductive database community  ( Ramakrishnan et al. 1992 )  . ',
    ' Machine Readable Dictionaries ( MRDs ) are a good source of lexical information and have been shown to be applicable to the task of LKB construction  ( Dolan et al. 1993  ,  Calzolari 1992  ,  Copestake 1990  ,  Wilks et al. 1989  ,  Byrd et al. 1987 )  .',
    'However , Pirkola  ( Pirkola 1998 )  , for example , used a subset of the TREC collection related to health topics , and showed that combination of general and domain specific ( i.e. , medical ) dictionaries improves the CLIR performance obtained with only a general dictionary .',
])}
ret = requests.post(url, data=data)
print(ret.text)

